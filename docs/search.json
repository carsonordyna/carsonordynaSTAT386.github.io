[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Carson Ordyna"
  },
  {
    "objectID": "about.html#background",
    "href": "about.html#background",
    "title": "About Me",
    "section": "Background",
    "text": "Background\nMy name is Carson Ordyna, and I am a student at Brigham Young University studying statistics with an emphasis in biostatistics. I am an aspiring physician and clinical researcher. I am passionate about harnessing the power of data science, machine learning, and AI to address challenges in the fields of medicine and healthcare. I am eager to collabroate on meaningful research projects aimed at improving human health and quality of life."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n\nStatistics (emphasis: Biostatistics) - Brigham Young University, 2026\nMathematics (minor)\nRelevant Coursework: Statistical Modeling, Probability and Inference, Mathematical Programming in Python, Statistical Computing in Epidemiology"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About Me",
    "section": "Experience",
    "text": "Experience\n\nResearch\n\nMassachusetts General Hospital: Clinical Research Assistant\n\nStudied the use of biometric data collected by patients’ smartwatches in the training of an algorithm that can predict postoperative complications\nAnalyzed patient adherence and generating activity reports with summary statistics\nWrote several Python scripts that automatically generate and share reports with team members on a daily/weekly basis \n\nBYU Computational Health Science Lab: Research Assistant\n\nDeveloped Python code to process and convert +30 million ICD-9 diagnosis codes to ICD-10 codes and map them to CCSR subcategory codes\nAnalyzed emergency room data using the Apriori algorithm to uncover correlations between CCSR codes and mental health diagnoses\nApplied Random Forest models to predict mental health-related occurrences in emergency room visits\n\nBYU School of Family Life: Research Assistant\n\nCollaborated to translate diagnostic surveys and questionnaires from English to Polish, ensuring linguistic accuracy and cultural adaptation\nProvided translations to support analysis of brief strategic therapy’s effectiveness in treating mental health disorders internationally\n\n\n\n\nVolunteer\n\nProject Read: Lab Leader and Volunteer\n\nParticipated in and lead weekly learning activities by which adults struggling with basic math and literacy can improve their knowledge and develop key life skills\n\nThe Church of Jesus Christ of Latter-day Saints: Full-time Representative in Poland\n\nReviewed weekly progress reports containing 8 performance metrics to identify areas needing improvement\nPlanned and conducted weekly/monthly training meetings for +25 volunteers to improve their skills and efficiency\nPersonally oversaw and approved monthly expenditures of +$2,000 of humanitarian relief for Ukrainian refugees and others in need\nServed as president of a small branch of 70 church members and attended to administrative, financial, and spiritual needs during Ukrainian refugee crisis"
  },
  {
    "objectID": "about.html#skills-interests",
    "href": "about.html#skills-interests",
    "title": "About Me",
    "section": "Skills & Interests",
    "text": "Skills & Interests\n\nTechnical Skills\n\nProgramming: Python, R, SAS\nData Analysis: Pandas, NumPy\nVisualization: Matplotlib, Seaborn, Tableau\nMachine Learning: Scikit-learn\nTools: Jupyter Notebooks, Git/GitHub\n\n\n\nAreas of Interest\n\nClinical and medical research (especially anything pertaining to surgery)\nMental health and epidemiological research\nCross-cultural projects\nPoland, Polish history, language, and culture"
  },
  {
    "objectID": "about.html#goals",
    "href": "about.html#goals",
    "title": "About Me",
    "section": "Goals",
    "text": "Goals\n\nUsing advancing data technologies to advance medical research, education, and practice in annovative ways that promote equity and quality of care\nExtracting insights from clinical data to inform creative solutions to complex issues in healthcare"
  },
  {
    "objectID": "about.html#get-to-know-me",
    "href": "about.html#get-to-know-me",
    "title": "About Me",
    "section": "Get to Know Me",
    "text": "Get to Know Me\nLately, my personal hobbies have been reading and watching college football and basketball. I am drawn to books about history, religion, psychology, medicine, and thought-provoking memoirs/biographies. Ultimately, though, I love spending time with my wife, Abi. We enjoy cooking and eating good food together and are always open to restaurant recommendations. We also love watching movies (especially true crime documentaries).\n\n\n\nMy Wife Abi and I"
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About Me",
    "section": "Contact",
    "text": "Contact\n\nEmail: carson.ordyna@gmail.com\nGitHub: https://github.com/carsonordyna\nLinkedIn: https://www.linkedin.com/in/carsonordyna\n\n\nThis portfolio showcases my learning progress and projects completed during my data science studies."
  },
  {
    "objectID": "about copy.html",
    "href": "about copy.html",
    "title": "About Me",
    "section": "",
    "text": "Carson Ordyna"
  },
  {
    "objectID": "about copy.html#background",
    "href": "about copy.html#background",
    "title": "About Me",
    "section": "Background",
    "text": "Background\nMy name is Carson Ordyna, and I am a student at Brigham Young University studying statistics with an emphasis in biostatistics. I am an aspiring physician and clinical researcher. I am passionate about harnessing the power of data science, machine learning, and AI to address challenges in the fields of medicine and healthcare. I am eager to collabroate on meaningful research projects aimed at improving human health and quality of life."
  },
  {
    "objectID": "about copy.html#education",
    "href": "about copy.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n\nStatistics (emphasis: Biostatistics) - Brigham Young University, 2026\nMathematics (minor)\nRelevant Coursework: Statistical Modeling, Probability and Inference, Mathematical Programming in Python, Statistical Computing in Epidemiology"
  },
  {
    "objectID": "about copy.html#experience",
    "href": "about copy.html#experience",
    "title": "About Me",
    "section": "Experience",
    "text": "Experience\n\nResearch\n\nMassachusetts General Hospital: Clinical Research Assistant\n\nStudied the use of biometric data collected by patients’ smartwatches in the training of an algorithm that can predict postoperative complications\nAnalyzed patient adherence and generating activity reports with summary statistics\nWrote several Python scripts that automatically generate and share reports with team members on a daily/weekly basis \n\nBYU Computational Health Science Lab: Research Assistant\n\nDeveloped Python code to process and convert +30 million ICD-9 diagnosis codes to ICD-10 codes and map them to CCSR subcategory codes\nAnalyzed emergency room data using the Apriori algorithm to uncover correlations between CCSR codes and mental health diagnoses\nApplied Random Forest models to predict mental health-related occurrences in emergency room visits\n\nBYU School of Family Life: Research Assistant\n\nCollaborated to translate diagnostic surveys and questionnaires from English to Polish, ensuring linguistic accuracy and cultural adaptation\nProvided translations to support analysis of brief strategic therapy’s effectiveness in treating mental health disorders internationally\n\n\n\n\nVolunteer\n\nProject Read: Lab Leader and Volunteer\n\nParticipated in and lead weekly learning activities by which adults struggling with basic math and literacy can improve their knowledge and develop key life skills\n\nThe Church of Jesus Christ of Latter-day Saints: Full-time Representative in Poland\n\nReviewed weekly progress reports containing 8 performance metrics to identify areas needing improvement\nPlanned and conducted weekly/monthly training meetings for +25 volunteers to improve their skills and efficiency\nPersonally oversaw and approved monthly expenditures of +$2,000 of humanitarian relief for Ukrainian refugees and others in need\nServed as president of a small branch of 70 church members and attended to administrative, financial, and spiritual needs during Ukrainian refugee crisis"
  },
  {
    "objectID": "about copy.html#skills-interests",
    "href": "about copy.html#skills-interests",
    "title": "About Me",
    "section": "Skills & Interests",
    "text": "Skills & Interests\n\nTechnical Skills\n\nProgramming: Python, R, SAS\nData Analysis: Pandas, NumPy\nVisualization: Matplotlib, Seaborn, Tableau\nMachine Learning: Scikit-learn\nTools: Jupyter Notebooks, Git/GitHub\n\n\n\nAreas of Interest\n\nClinical and medical research (especially anything pertaining to surgery)\nMental health and epidemiological research\nCross-cultural projects\nPoland, Polish history, language, and culture"
  },
  {
    "objectID": "about copy.html#goals",
    "href": "about copy.html#goals",
    "title": "About Me",
    "section": "Goals",
    "text": "Goals\n\nUsing advancing data technologies to advance medical research, education, and practice in annovative ways that promote equity and quality of care\nExtracting insights from clinical data to inform creative solutions to complex issues in healthcare"
  },
  {
    "objectID": "about copy.html#get-to-know-me",
    "href": "about copy.html#get-to-know-me",
    "title": "About Me",
    "section": "Get to Know Me",
    "text": "Get to Know Me\nLately, my personal hobbies have been reading and watching college football and basketball. I am drawn to books about history, religion, psychology, medicine, and thought-provoking memoirs/biographies. Ultimately, though, I love spending time with my wife, Abi. We enjoy cooking and eating good food together and are always open to restaurant recommendations. We also love watching movies (especially true crime documentaries).\n\n\n\nMy Wife Abi and I"
  },
  {
    "objectID": "about copy.html#contact",
    "href": "about copy.html#contact",
    "title": "About Me",
    "section": "Contact",
    "text": "Contact\n\nEmail: carson.ordyna@gmail.com\nGitHub: https://github.com/carsonordyna\nLinkedIn: https://www.linkedin.com/in/carsonordyna\n\n\nThis portfolio showcases my learning progress and projects completed during my data science studies."
  },
  {
    "objectID": "projects/final-project.html",
    "href": "projects/final-project.html",
    "title": "Final Project",
    "section": "",
    "text": "This is coming down the pipeline. Check again later.",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "projects/data-acquisition.html",
    "href": "projects/data-acquisition.html",
    "title": "Data Acquisition Project",
    "section": "",
    "text": "Scraping LDS Scripture Count and Citation Data\n\nOutline\n\nIntroduction\nMethod\nExploratory Data Analysis\nAdditional Information\n\n\n\n\nLogo for The Church of Jesus Christ of Latter-day Saints\n\n\n\n\nIntroduction\nFor two years, from 2021 to 2023, I served as a full-time volunteer for my church, The Church of Jesus Christ of Latter-day Saints in Poland. Every six months, leaders of our Church give talks about Jesus Christ and His restored gospel, and members across the world listen to the broadcasted speeches. As a volunteer, I spent at least an hour each day studying from the Bible, the Book of Mormon, and other books of scripture, and I often wondered which verses of scripture were most often cited in the talks given by church leaders. I figured that a given verse’s citation frequency was a good proxy measure of its importance. If certain verses have been quoted rather frequently, then I felt that I should know the references to them—and maybe even memorize the verses themselves.\nThe issue, though, was having the time and willpower to manually comb through each talk and tally up all the scripture citations. In my eyes, the juice simply wasn’t worth the squeeze. However, after returning home from Poland, through my statistics coursework, I have learned webscraping skills that have allowed me automate this task, thereby providing the desired results in a fraction of the time and with a fraction of the effort that would have been required to do everything manually.\n\n\n\nScriptures\n\n\n\n\nMethod\nThe semiannual meetings during which leaders give these talks are called General Conferences. On the Church’s website, General Conference talks from the last several years may be found. Additionally, the content of all five books of the Church’s cannonized scripture (the Old Testament, New Testament, Book of Mormon, Doctrine and Covenants, and the Pearl of Great Price, collectively referred to as the “Standard Works”) may also be found on the Church’s website. After reviewing the robots.txt for the site, I confirmed that the general-conference and scripture webpages were not disallowed and, therefore, were eligible for scraping.\nI began with obtaining a list of verse counts for each chapter in each book in each standard work. These verse counts were then used when whole chapters and whole books were cited. In such cases, each verse in the whole chapter/book were counted as being cited. To generate this list of verse counts, I explored the scripture pages of the Church’s website and made note of how the web address was patterned. I observed that the address format of a webpage for a given chapter was: https://www.churchofjesuschrist.org/study/scriptures/{standard work code}/{book code}/{chapter number}. From the main scripture page, using requests and BeautifulSoup, I scraped a list of these codes for each standard work (e.g. bofm for Book of Mormon) and each book within it (e.g. 1-ne for 1 Nephi). I was then able to iterate through these standard work and book codes, scrape the webpages for all the chapters, and ultimately record the total number of verses in each chapter.\nAfter obtaining verse count data, I explored the general-conference pages of the Church’s website. As similarly done before, I made note of the address format of a webpage for a given General Conference talk. Then, I began to iterate through each year (from 2016 to 2025 inclusive) and through each of the two conferences held each year. For each conference, I iterated through each talk given, and for each talk, I scraped a list of all links that were to a scripture webpage. I filtered out ones that were not associated with a legitimate verse, set of verses, chapter, or book, and I appended the rest to a running list. For simplicity, I broke up sets of verses individually (e.g. 1 Nephi 4:6-7 would be treated as 1 Nephi 4:6 and 1 Nephi 4:7). Entire chapters and books were treated similarly.\n\n\n\nDallin H. Oaks, President of the Church, speaking at General Conference (Oct. 2025)\n\n\nAfter iterating through everything, the running list of citations were grouped by Book, Chapter, and Verse and the number of Citations computed. The data was then subsetted to include only verses that were cited at least 8 times (\\(n = 887\\)). For this subset, the corresponding text of each verse was also scraped and added. This citation data was merged with the count data and then saved.\nThe following consists of some brief exploratory analysis of the results.\n\n\nExploratory Data Analysis\n\n\nCode\n# Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tableone import TableOne\n\n# Read in the data\ndf = pd.read_csv(\"../verse_data/Verses.csv\")\ncounts_df = pd.read_csv(\"../verse_data/VerseCounts.csv\")\n\n# Add two columns with individual book and standard work verse counts\ndf['Book_Verse_Count'] = df['Book'].map(counts_df.groupby(by='Book')['Verse_Count'].sum().to_dict())\ndf['Standard_Work_Verse_Count'] = df['Standard_Work'].map(counts_df.groupby(by='Standard_Work')['Verse_Count'].sum().to_dict())\n\n# Inspect the data\nprint(f\"{df.shape[0]} rows and {df.shape[1]} columns \\n\")\nprint(\"The 15 most cited verses in the last 10 years:\")\ndf[['Book', 'Chapter', 'Verse', 'Citations', 'Text']].head(15)\n\n\n887 rows and 9 columns \n\nThe 15 most cited verses in the last 10 years:\n\n\n\n\n\n\n\n\n\nBook\nChapter\nVerse\nCitations\nText\n\n\n\n\n0\n2 Nephi\n31\n20\n68\nWherefore, ye must press forward with a steadf...\n\n\n1\nAlma\n7\n12\n64\nAnd he will take upon him death, that he may l...\n\n\n2\nAlma\n7\n11\n61\nAnd he shall go forth, suffering pains and aff...\n\n\n3\nMoses\n1\n39\n59\nFor behold, this is my work and my glory—to br...\n\n\n4\nMatthew\n11\n28\n52\nCome unto me, all ye that labour and are heavy...\n\n\n5\nDoctrine and Covenants\n20\n77\n52\nO God, the Eternal Father, we ask thee in the ...\n\n\n6\nMosiah\n18\n9\n50\nYea, and are willing to mourn with those that ...\n\n\n7\nMosiah\n2\n41\n47\nAnd moreover, I would desire that ye should co...\n\n\n8\nJohn\n3\n16\n47\nFor God so loved the world, that he gave his o...\n\n\n9\nMoroni\n10\n32\n47\nYea, come unto Christ, and be perfected in him...\n\n\n10\nMosiah\n3\n19\n46\nFor the natural man is an enemy to God, and ha...\n\n\n11\nMatthew\n22\n37\n45\nJesus said unto him, Thou shalt love the Lord ...\n\n\n12\nDoctrine and Covenants\n84\n20\n43\nTherefore, in the ordinances thereof, the powe...\n\n\n13\nMatthew\n22\n39\n43\nAnd the second is like unto it, Thou shalt lov...\n\n\n14\n2 Nephi\n31\n19\n42\nAnd now, my beloved brethren, after ye have go...\n\n\n\n\n\n\n\n\n\nCode\n# 1. Display a \"Table 1\" with standard works and citation COUNTS\n\n# Generate the table for COUNTS\ntable_df = TableOne(df, columns=['Standard_Work', 'Citations'],\n                    categorical=['Standard_Work'], continuous=['Citations'], missing = False).tableone\n\n# Drop the last row and rename the index and a column\ntable_df = table_df.iloc[:-1, ]\ntable_df.columns = ['Citations']\ntable_df.index = pd.MultiIndex.from_tuples([(                   'n',                       ''),\n                                            ('Standard Work: n (%)',         'Book of Mormon'),\n                                            ('Standard Work: n (%)', 'Doctrine and Covenants'),\n                                            ('Standard Work: n (%)',          'New Testament'),\n                                            ('Standard Work: n (%)',          'Old Testament'),\n                                            ('Standard Work: n (%)',   'Pearl of Great Price')])\n\n# Calculate the PROPORTIONS (i.e. average citations per verse)\ndf_1 = df.merge(counts_df, on=[\"Standard_Work\", \"Book\", \"Chapter\"])\ncite_dict = df_1.groupby('Standard_Work')['Citations'].sum().to_dict()\ncount_dict = counts_df.groupby(by='Standard_Work')['Verse_Count'].sum().to_dict()\ndf_1 = {}\nfor key in cite_dict:\n    df_1[key] = round(cite_dict[key] / count_dict[key], 2)\n\n# Add the PROPORTIONS (i.e. average citations per verse) to the table\ntable_df['Average Citations per Verse'] = [df_1.get(idx[1], '') for idx in table_df.index]\n\n# Reorder the rows\ntable_df.iloc[[0, 1, 3, 2, 5, 4]]\n\n\n\n\n\n\n\n\n\n\nCitations\nAverage Citations per Verse\n\n\n\n\nn\n\n887\n\n\n\nStandard Work: n (%)\nBook of Mormon\n332 (37.4)\n0.75\n\n\nNew Testament\n254 (28.6)\n0.39\n\n\nDoctrine and Covenants\n226 (25.5)\n0.75\n\n\nPearl of Great Price\n56 (6.3)\n1.07\n\n\nOld Testament\n19 (2.1)\n0.01\n\n\n\n\n\n\n\n\n\nCode\n# 2. Bar chart with the books with the most citations (by proportion) \n\n# Prepare the data\ndf_2 = {}\ncite_dict = df.groupby('Book')['Citations'].sum().to_dict()\ncount_dict = counts_df.groupby(by='Book')['Verse_Count'].sum().to_dict()\nfor key in cite_dict:\n    df_2[key] = round(cite_dict[key] / count_dict[key], 2)\ndf_2 = pd.DataFrame(list(df_2.items()), columns=['Book', 'Average'])\ndf_2 = df_2.sort_values(by='Average', ascending=False).head(7)  # Books with average citations per verse greater than one\ndf_2.reset_index(drop = True, inplace = True)\n\n# Make and display the plot\nplt.figure(figsize=(8,5))\nplt.bar(df_2[\"Book\"], df_2[\"Average\"], color = 'b')\nplt.xticks(rotation=45, ha='right')\nplt.ylim(0, 4.5)\nplt.xlabel(\"Book\")\nplt.ylabel(\"Citations per Verse\")\nplt.title(\"Average Citations per Verse (by Book)\")\nplt.text(4, 3.95, \"Books with average citations \\n per verse greater than one\", ha = 'left', fontsize = 10) \nfor bar in plt.bar(df_2[\"Book\"], df_2[\"Average\"]):\n    height = bar.get_height()\n    plt.text(\n        bar.get_x() + bar.get_width()/2,\n        height,                   \n        f\"{height:.2f}\",        \n        ha='center', va='bottom'     \n    )\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nI was surprised to see that 2 Nephi 31:20 has been the most cited verse in General Conference over the last 10 years. It is certainly a powerful verse, but I did not expect to see it at the top spot. When I asked friends and family what they thought would be number one, many of them guessed 1 Nephi 3:7, John 3:16, and others, but no one thought of 2 Nephi 31:20. After examining the most-cited verses, many come with little surprise. For example, the gospel truths taught in Alma 7:11-12, Moses 1:39, and in Matthew 11:28 are equally comforting and inspiring—and ultimately worthy of every reference.\nI also wanted to know which standard works and individual books were cited the most—by both count and proportion. It was expected that the Book of Mormon receive a lot of attention, whereas the Old Testament receive comparatively less so. What was interesting to see was that the verses in Enos, the Articles of Faith, and Joseph Smith—History were all cited, on average, more than once, placing such books in the top seven. I would have expected one of the four gospels to make an appearance (e.g. John), but such is not the case.\nFuther analysis may be done in an attempt to discover textual patterns in the most-cited verses, which may provide additional insight.\n\n\nAdditional Information\nThe code and outputted data associated with this analysis and its results may be found in this Github repository. The README.md file in the repository briefly outlines the various files found therein. Additional information and context may be gained through exploration of the various pages of the Church’s website.",
    "crumbs": [
      "Data Acquisition Project"
    ]
  },
  {
    "objectID": "tutorial_blog.html",
    "href": "tutorial_blog.html",
    "title": "Tutorial Blog",
    "section": "",
    "text": "The Cox Proportional Hazards Model: A Brief Introduction and Demo\n\nOutline\n\nIntroduction\nThe Mathematical Model\nA Brief Demo\nConclusion\n\n\n\n\nA doctor’s stethoscope\n\n\n\n\nIntroduction\nIn clinical research, it is valuable to be able to model and predict whether a patient will experience a specified event (e.g. death, the return of a disease like cancer, etc.). Just as important is being able to model and predict when such an event occurs (e.g. 3 years after the diagnosis of a cancer at a particular stage). This is a foundational focus of survival analysis: the use of statistical models trained on time-to-event data. One of the most common and powerful tools in survival analysis is the Cox Proportional Hazards Model (Cox model for short), which helps us understand how different factors (age, treatment, comorbidities, etc.) influence a given individual’s changing, moment-to-moment risk of experiencing a given event over a period of time.\nFor the last several months, I have had the privilege of working on a clinical research team at Massachusetts General Hospital. As I have reviewed scientific literature regarding lung cancer and surgical pproaches to treating it, I have seen firsthand how prevelent this model is in medical/clinical research. This post offers a brief introduction to the Cox model and a simple, hands-on demo using Python.\n\n\nThe Mathematical Model\nAt the core of the Cox model is the hazard function. A hazard function, \\(\\lambda (t)\\), is one that indicates the instantaneous rate of an event occurring—in other words, the instantaneous risk of experiencing the event (e.g. death)—at a specific time, \\(t\\). The baseline hazard function \\(\\lambda_0 (t)\\) refers to the hazard function when explanatory variables (i.e. covariates), \\(X\\), are at their baseline levels (e.g. when no treatment/medication is given, when a particular comorbidity is not present, etc.). That is, \\(X_i = 0\\) for all \\(i \\in \\{1, 2, ..., n \\}\\) for \\(n\\) covariates. The effect of each covariate is denoted as \\(\\beta\\) with \\(\\beta_i\\) denoting the effect of the \\(i\\)-th covariate. The overall hazard function, \\(\\lambda(t \\mid X)\\), is as follows:\n\\[\n\\lambda(t \\mid X) = \\lambda_0 (t) e^{\\displaystyle \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_n X_n}\n\\]\nwhere \\(X =\\) \\((X_1, X_2, ..., X_n)\\) is the covariate vector. Note that if \\(\\beta_i &gt; 0\\), then increasing \\(X_i\\) (or \\(X_i = 1\\) if binary) means an increased risk, and if \\(\\beta_i &lt; 0\\), then increasing \\(X_i\\) (or \\(X_i = 1\\) if binary) means a decreased risk. In the case of a single covariate, \\(\\beta_1\\), it can be shown that\n\\[\n\\frac{\\lambda(t \\mid x + 1)}{\\lambda(t \\mid x)} = e^{\\displaystyle \\beta_1}\n\\]\nWhen comparing two patients, \\(a\\) and \\(b\\), in the single covariate case, it can also be shown that\n\\[\n\\frac{\\lambda(t \\mid X_a)}{\\lambda(t \\mid X_b)} = e^{\\displaystyle \\beta_1(X_a - X_b)}\n\\]\nThe right-hand side of these equations are constants independent of time, \\(t\\). These presence of these constant proportions are the reason why the model is called the Cox Proportional Hazards model. The derivation of these results and deeper mathematical theory and background behind the model is beyond the scope of this blog. More information may be found here.\n\n\n\nHazard Function Graph\n\n\n\n\nA Brief Demo\nNow, I’ll present a simple example (using the lifelines Python library, link to documentation) of how to fit and interpret a Cox model on a small, synthetic dataset.\n\n# Install \"lifelines\" library if not already installed\n!pip install lifelines\n\n\n# Import libraries\n#| warning: false\nimport pandas as pd\nfrom lifelines import CoxPHFitter\n\n# Sample dataset\ndata = pd.DataFrame({\n    'age': [79, 50, 45, 70, 65, 70, 53, 66, 50, 77, 58, 70, 53, 56, 71, 76,\n            60, 75, 68, 76],\n    'treatment': [0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1],\n    'gender': [1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0],\n    'duration': [1, 5, 2, 3, 5, 2, 5, 5, 5, 2, 5, 5, 4, 2, 5, 5, 4, 1, 5, 5],\n    'event': [1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1]\n})\n\n# Fit the Cox model\ncph = CoxPHFitter()\ncph.fit(data, duration_col='duration', event_col='event')\n\n# Examine the results\nsummary = cph.summary[[\"coef\", \"p\", \"coef lower 95%\", \"coef upper 95%\"]]\nsummary\n\n\n\n\n\n\n\n\ncoef\np\ncoef lower 95%\ncoef upper 95%\n\n\ncovariate\n\n\n\n\n\n\n\n\nage\n0.124049\n0.026572\n0.014419\n0.233679\n\n\ntreatment\n-2.023659\n0.018711\n-3.710554\n-0.336763\n\n\ngender\n0.272174\n0.736929\n-1.315843\n1.860192\n\n\n\n\n\n\n\n In the summary table, at the \\(\\alpha = 0.05\\) level, we observe evidence that the \\(\\beta\\) for age and treatment is statistically different from 0. For age, the estimate of \\(\\beta\\) is positive, indicating that older individuals exhibit a higher instantaneous risk of experiencing the specified event (e.g. death). For treatment, we see that the estimate for \\(\\beta\\) is negative, indicating that individuals who receive the specified treatment (i.e. treatment = 1) exhibit a lower instantaneous risk of experiencing the specified event. Also note that the \\(\\beta\\) for gender is not statistically different from 0, indicating that gender probably does not influence an individual’s risk.\n\n\nConclusion\nTo summarize, the Cox Proportional Hazards Model is a useful tool for analyzing time-to-event data, especially in healthcare and clinical research. It allows us to quantify how different factors influence the timing of critical events.\nI invite you to further study the math behind the model and/or find a dataset of interest (e.g. from Kaggle’s medical datasets) and try fitting a Cox model using lifelines. After interpreting the results, you may consder sharing your findings in a GitHub repo or in a blog post like this one."
  },
  {
    "objectID": "index copy.html",
    "href": "index copy.html",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "Welcome to my data science portfolio! My name is Carson Ordyna, and I am currently a senior at Brigham Young University studying statistics. This site is an on-going project that will serve as a platform where I hope to showcase my learning, growth, and other projects that I have (or are currently) working on. Most of the content on this site will involve data science and analytics. At least for the current moment, I expect that the majority of the site’s material will result from and/or be motivated by assignments in my STAT 386 (Data Science Process) course. I hope you will find my work interesting and insightful!\n\n\n\nBYU Statistics Department Logo\n\n\n\n\nThis portfolio shows my work and efforts to learn and gain skills in the field of data science. For project that I display, I hope to include:\n\nBoth my code, detailed comments, and documentation\nAny visualizations, plots, or figures that I have created\nReflections, inights, and conclusions regarding the project itself and what I learned and discovered in the process\n\nIn addition to my projects, one may also find my biographical information, blog plots I have written, and hopefully other fascinating cotent.\n\n\n\n\nProgramming: Python, Pandas, NumPy, and writing efficient, interpretable code\nVisualization: Creating plots with Matplotlib, Seaborn, and potentially other data visualization packages\nData Collection: Aquiring data from files, websites, and APIs\nData Cleaning: Thinking thoughtfully about missing data and how to best handle it in terms of the data itself, the analysis, and the question(s) motivating it\nAnalysis: Selecting appropriate statistical and machine learning models, evaluating their performance, and soundly interpreting their results\nDiscovery: Using data to find patterns, answer questions, and fuel innovative solutions\n\n\n\n\n\n\n\n(Coming soon!) This is a future STAT 386 class project where I will explore datasets to find interesting patterns and answer questions.\n\n\n\n(Coming soon!) This is a future STAT 386 class project where I will gather data from different sources and prepare it for analysis.\n\n\n\n(Coming soon!) This is will be the final project for my STAT 386 class where I will engage in the data science process from beginning to end.\n\n\n\nI built this site using Quarto and host it on GitHub Pages."
  },
  {
    "objectID": "index copy.html#about-this-portfolio",
    "href": "index copy.html#about-this-portfolio",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "This portfolio shows my work and efforts to learn and gain skills in the field of data science. For project that I display, I hope to include:\n\nBoth my code, detailed comments, and documentation\nAny visualizations, plots, or figures that I have created\nReflections, inights, and conclusions regarding the project itself and what I learned and discovered in the process\n\nIn addition to my projects, one may also find my biographical information, blog plots I have written, and hopefully other fascinating cotent."
  },
  {
    "objectID": "index copy.html#skills-im-learning",
    "href": "index copy.html#skills-im-learning",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "Programming: Python, Pandas, NumPy, and writing efficient, interpretable code\nVisualization: Creating plots with Matplotlib, Seaborn, and potentially other data visualization packages\nData Collection: Aquiring data from files, websites, and APIs\nData Cleaning: Thinking thoughtfully about missing data and how to best handle it in terms of the data itself, the analysis, and the question(s) motivating it\nAnalysis: Selecting appropriate statistical and machine learning models, evaluating their performance, and soundly interpreting their results\nDiscovery: Using data to find patterns, answer questions, and fuel innovative solutions"
  },
  {
    "objectID": "index copy.html#my-projects",
    "href": "index copy.html#my-projects",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "(Coming soon!) This is a future STAT 386 class project where I will explore datasets to find interesting patterns and answer questions.\n\n\n\n(Coming soon!) This is a future STAT 386 class project where I will gather data from different sources and prepare it for analysis.\n\n\n\n(Coming soon!) This is will be the final project for my STAT 386 class where I will engage in the data science process from beginning to end.\n\n\n\nI built this site using Quarto and host it on GitHub Pages."
  },
  {
    "objectID": "projects/eda.html",
    "href": "projects/eda.html",
    "title": "EDA Project",
    "section": "",
    "text": "This is coming down the pipeline. Check again later.",
    "crumbs": [
      "EDA Project"
    ]
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects Overview",
    "section": "",
    "text": "(Coming soon!) Description: Pick a dataset and explore it to discover insights and answer questions.\n\n\n\n(Coming soon!) Description: Find an interesting data source, collect the data, and prepare it for analysis.\n\n\n\n(Coming soon!) Description: A comprehensive project that shows off my data science skills.",
    "crumbs": [
      "Projects Overview"
    ]
  },
  {
    "objectID": "projects/index.html#all-projects",
    "href": "projects/index.html#all-projects",
    "title": "Projects Overview",
    "section": "",
    "text": "(Coming soon!) Description: Pick a dataset and explore it to discover insights and answer questions.\n\n\n\n(Coming soon!) Description: Find an interesting data source, collect the data, and prepare it for analysis.\n\n\n\n(Coming soon!) Description: A comprehensive project that shows off my data science skills.",
    "crumbs": [
      "Projects Overview"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "Welcome to my data science portfolio! My name is Carson Ordyna, and I am currently a senior at Brigham Young University studying statistics. This site is an on-going project that will serve as a platform where I hope to showcase my learning, growth, and other projects that I have (or are currently) working on. Most of the content on this site will involve data science and analytics. At least for the current moment, I expect that the majority of the site’s material will result from and/or be motivated by assignments in my STAT 386 (Data Science Process) course. I hope you will find my work interesting and insightful!\n\n\n\nBYU Statistics Department Logo\n\n\n\n\nThis portfolio shows my work and efforts to learn and gain skills in the field of data science. For projects that I display, I hope to include:\n\nMy code, detailed comments, and documentation\nAny visualizations, plots, or figures that I have created\nReflections, insights, and conclusions regarding the project itself and what I learned and discovered in the process\n\nIn addition to my projects, one may also find my biographical information, blog posts I have written, and hopefully other fascinating content.\n\n\n\n\nProgramming: Python, Pandas, NumPy, and writing efficient, interpretable code\nVisualization: Creating plots with Matplotlib, Seaborn, and potentially other data visualization packages\nData Collection: Aquiring data from files, websites, and APIs\nData Cleaning: Thinking thoughtfully about missing data and how to best handle it in terms of the data itself, the analysis, and the question(s) motivating it\nAnalysis: Selecting appropriate statistical and machine learning models, evaluating their performance, and soundly interpreting their results\nDiscovery: Using data to find patterns, answer questions, and fuel innovative solutions\n\n\n\n\n\n\n\n(Coming soon!) This is a future STAT 386 class project where I will explore datasets to find interesting patterns and answer questions.\n\n\n\n(Coming soon!) This is a future STAT 386 class project where I will gather data from different sources and prepare it for analysis.\n\n\n\n(Coming soon!) This is will be the final project for my STAT 386 class where I will engage in the data science process from beginning to end.\n\n\n\nI built this site using Quarto and host it on GitHub Pages."
  },
  {
    "objectID": "index.html#about-this-portfolio",
    "href": "index.html#about-this-portfolio",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "This portfolio shows my work and efforts to learn and gain skills in the field of data science. For projects that I display, I hope to include:\n\nMy code, detailed comments, and documentation\nAny visualizations, plots, or figures that I have created\nReflections, insights, and conclusions regarding the project itself and what I learned and discovered in the process\n\nIn addition to my projects, one may also find my biographical information, blog posts I have written, and hopefully other fascinating content."
  },
  {
    "objectID": "index.html#skills-im-learning",
    "href": "index.html#skills-im-learning",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "Programming: Python, Pandas, NumPy, and writing efficient, interpretable code\nVisualization: Creating plots with Matplotlib, Seaborn, and potentially other data visualization packages\nData Collection: Aquiring data from files, websites, and APIs\nData Cleaning: Thinking thoughtfully about missing data and how to best handle it in terms of the data itself, the analysis, and the question(s) motivating it\nAnalysis: Selecting appropriate statistical and machine learning models, evaluating their performance, and soundly interpreting their results\nDiscovery: Using data to find patterns, answer questions, and fuel innovative solutions"
  },
  {
    "objectID": "index.html#my-projects",
    "href": "index.html#my-projects",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "(Coming soon!) This is a future STAT 386 class project where I will explore datasets to find interesting patterns and answer questions.\n\n\n\n(Coming soon!) This is a future STAT 386 class project where I will gather data from different sources and prepare it for analysis.\n\n\n\n(Coming soon!) This is will be the final project for my STAT 386 class where I will engage in the data science process from beginning to end.\n\n\n\nI built this site using Quarto and host it on GitHub Pages."
  }
]